{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from pygcn.utils import load_data, accuracy\n",
    "from pygcn.models import GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n",
      "Data loaded.\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "adj, features, labels, idx_train, idx_val, idx_test = load_data(path = \"data/cora/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2708, 1433])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nfeatures: 1433\n",
      "nHidden: 16\n",
      "nClass: 7\n"
     ]
    }
   ],
   "source": [
    "hidden = 16 \n",
    "lr = 0.01\n",
    "epochs = 200\n",
    "dropout = 0.5\n",
    "weight_decay = 5e-4\n",
    "seed = 42\n",
    "\n",
    "print('nfeatures:', features.shape[1])\n",
    "print('nHidden:', hidden)\n",
    "print('nClass:', labels.max().item() + 1)\n",
    "model = GCN(nfeat=features.shape[1],\n",
    "            nhid=hidden,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=dropout)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=lr, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    print('Using GPU.')\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    model.cuda()\n",
    "    features = features.cuda()\n",
    "    adj = adj.cuda()\n",
    "    labels = labels.cuda()\n",
    "    idx_train = idx_train.cuda()\n",
    "    idx_val = idx_val.cuda()\n",
    "    idx_test = idx_test.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, fastmode=False):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if not fastmode:\n",
    "        # Evaluate validation set performance separately,\n",
    "        # deactivates dropout during validation run.\n",
    "        model.eval()\n",
    "        output = model(features, adj)\n",
    "\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 1.9113 acc_train: 0.2643 loss_val: 1.8903 acc_val: 0.3867 time: 0.3067s\n",
      "Epoch: 0002 loss_train: 1.8874 acc_train: 0.2786 loss_val: 1.8776 acc_val: 0.4133 time: 0.0109s\n",
      "Epoch: 0003 loss_train: 1.8759 acc_train: 0.3429 loss_val: 1.8658 acc_val: 0.4300 time: 0.0088s\n",
      "Epoch: 0004 loss_train: 1.8640 acc_train: 0.3214 loss_val: 1.8547 acc_val: 0.3767 time: 0.0084s\n",
      "Epoch: 0005 loss_train: 1.8542 acc_train: 0.3143 loss_val: 1.8439 acc_val: 0.3667 time: 0.0081s\n",
      "Epoch: 0006 loss_train: 1.8326 acc_train: 0.3643 loss_val: 1.8333 acc_val: 0.3533 time: 0.0089s\n",
      "Epoch: 0007 loss_train: 1.8318 acc_train: 0.2786 loss_val: 1.8229 acc_val: 0.3500 time: 0.0086s\n",
      "Epoch: 0008 loss_train: 1.8299 acc_train: 0.3286 loss_val: 1.8128 acc_val: 0.3500 time: 0.0084s\n",
      "Epoch: 0009 loss_train: 1.8185 acc_train: 0.3000 loss_val: 1.8031 acc_val: 0.3500 time: 0.0089s\n",
      "Epoch: 0010 loss_train: 1.7992 acc_train: 0.3000 loss_val: 1.7937 acc_val: 0.3500 time: 0.0093s\n",
      "Epoch: 0011 loss_train: 1.7716 acc_train: 0.3286 loss_val: 1.7846 acc_val: 0.3500 time: 0.0095s\n",
      "Epoch: 0012 loss_train: 1.7923 acc_train: 0.3214 loss_val: 1.7763 acc_val: 0.3500 time: 0.0089s\n",
      "Epoch: 0013 loss_train: 1.7625 acc_train: 0.3357 loss_val: 1.7685 acc_val: 0.3500 time: 0.0088s\n",
      "Epoch: 0014 loss_train: 1.7630 acc_train: 0.3143 loss_val: 1.7612 acc_val: 0.3500 time: 0.0081s\n",
      "Epoch: 0015 loss_train: 1.7647 acc_train: 0.3071 loss_val: 1.7544 acc_val: 0.3500 time: 0.0085s\n",
      "Epoch: 0016 loss_train: 1.7392 acc_train: 0.3000 loss_val: 1.7480 acc_val: 0.3500 time: 0.0084s\n",
      "Epoch: 0017 loss_train: 1.7506 acc_train: 0.3071 loss_val: 1.7420 acc_val: 0.3500 time: 0.0084s\n",
      "Epoch: 0018 loss_train: 1.7222 acc_train: 0.3000 loss_val: 1.7363 acc_val: 0.3500 time: 0.0088s\n",
      "Epoch: 0019 loss_train: 1.7326 acc_train: 0.3000 loss_val: 1.7307 acc_val: 0.3500 time: 0.0105s\n",
      "Epoch: 0020 loss_train: 1.7208 acc_train: 0.3071 loss_val: 1.7252 acc_val: 0.3500 time: 0.0091s\n",
      "Epoch: 0021 loss_train: 1.6985 acc_train: 0.3143 loss_val: 1.7194 acc_val: 0.3500 time: 0.0085s\n",
      "Epoch: 0022 loss_train: 1.7330 acc_train: 0.3000 loss_val: 1.7137 acc_val: 0.3500 time: 0.0085s\n",
      "Epoch: 0023 loss_train: 1.6831 acc_train: 0.3143 loss_val: 1.7078 acc_val: 0.3500 time: 0.0084s\n",
      "Epoch: 0024 loss_train: 1.6884 acc_train: 0.3143 loss_val: 1.7016 acc_val: 0.3500 time: 0.0076s\n",
      "Epoch: 0025 loss_train: 1.6745 acc_train: 0.3000 loss_val: 1.6953 acc_val: 0.3500 time: 0.0077s\n",
      "Epoch: 0026 loss_train: 1.6584 acc_train: 0.3143 loss_val: 1.6887 acc_val: 0.3500 time: 0.0081s\n",
      "Epoch: 0027 loss_train: 1.6698 acc_train: 0.3357 loss_val: 1.6818 acc_val: 0.3500 time: 0.0084s\n",
      "Epoch: 0028 loss_train: 1.6523 acc_train: 0.3429 loss_val: 1.6746 acc_val: 0.3500 time: 0.0086s\n",
      "Epoch: 0029 loss_train: 1.6214 acc_train: 0.3357 loss_val: 1.6672 acc_val: 0.3567 time: 0.0085s\n",
      "Epoch: 0030 loss_train: 1.6267 acc_train: 0.3571 loss_val: 1.6596 acc_val: 0.3633 time: 0.0084s\n",
      "Epoch: 0031 loss_train: 1.5986 acc_train: 0.3857 loss_val: 1.6515 acc_val: 0.3633 time: 0.0080s\n",
      "Epoch: 0032 loss_train: 1.6018 acc_train: 0.3857 loss_val: 1.6432 acc_val: 0.3667 time: 0.0088s\n",
      "Epoch: 0033 loss_train: 1.6010 acc_train: 0.3714 loss_val: 1.6344 acc_val: 0.3700 time: 0.0086s\n",
      "Epoch: 0034 loss_train: 1.6138 acc_train: 0.4000 loss_val: 1.6255 acc_val: 0.3700 time: 0.0085s\n",
      "Epoch: 0035 loss_train: 1.5626 acc_train: 0.4357 loss_val: 1.6160 acc_val: 0.3700 time: 0.0110s\n",
      "Epoch: 0036 loss_train: 1.5185 acc_train: 0.4214 loss_val: 1.6059 acc_val: 0.3767 time: 0.0099s\n",
      "Epoch: 0037 loss_train: 1.5640 acc_train: 0.4357 loss_val: 1.5955 acc_val: 0.3800 time: 0.0088s\n",
      "Epoch: 0038 loss_train: 1.5486 acc_train: 0.4643 loss_val: 1.5845 acc_val: 0.4033 time: 0.0088s\n",
      "Epoch: 0039 loss_train: 1.4991 acc_train: 0.4500 loss_val: 1.5732 acc_val: 0.4033 time: 0.0087s\n",
      "Epoch: 0040 loss_train: 1.4923 acc_train: 0.4357 loss_val: 1.5616 acc_val: 0.4100 time: 0.0086s\n",
      "Epoch: 0041 loss_train: 1.4669 acc_train: 0.4714 loss_val: 1.5498 acc_val: 0.4200 time: 0.0083s\n",
      "Epoch: 0042 loss_train: 1.4881 acc_train: 0.4500 loss_val: 1.5381 acc_val: 0.4400 time: 0.0087s\n",
      "Epoch: 0043 loss_train: 1.4895 acc_train: 0.4714 loss_val: 1.5261 acc_val: 0.4633 time: 0.0090s\n",
      "Epoch: 0044 loss_train: 1.4357 acc_train: 0.4857 loss_val: 1.5141 acc_val: 0.4767 time: 0.0092s\n",
      "Epoch: 0045 loss_train: 1.4070 acc_train: 0.5071 loss_val: 1.5019 acc_val: 0.4867 time: 0.0087s\n",
      "Epoch: 0046 loss_train: 1.3969 acc_train: 0.5143 loss_val: 1.4898 acc_val: 0.4933 time: 0.0085s\n",
      "Epoch: 0047 loss_train: 1.3750 acc_train: 0.5286 loss_val: 1.4775 acc_val: 0.5200 time: 0.0093s\n",
      "Epoch: 0048 loss_train: 1.3585 acc_train: 0.5357 loss_val: 1.4650 acc_val: 0.5400 time: 0.0090s\n",
      "Epoch: 0049 loss_train: 1.3662 acc_train: 0.5429 loss_val: 1.4525 acc_val: 0.5600 time: 0.0088s\n",
      "Epoch: 0050 loss_train: 1.3140 acc_train: 0.6000 loss_val: 1.4398 acc_val: 0.5667 time: 0.0088s\n",
      "Epoch: 0051 loss_train: 1.3111 acc_train: 0.6143 loss_val: 1.4264 acc_val: 0.5700 time: 0.0089s\n",
      "Epoch: 0052 loss_train: 1.2952 acc_train: 0.6286 loss_val: 1.4128 acc_val: 0.5833 time: 0.0082s\n",
      "Epoch: 0053 loss_train: 1.2787 acc_train: 0.6571 loss_val: 1.3990 acc_val: 0.5867 time: 0.0088s\n",
      "Epoch: 0054 loss_train: 1.2604 acc_train: 0.6214 loss_val: 1.3852 acc_val: 0.5933 time: 0.0085s\n",
      "Epoch: 0055 loss_train: 1.2539 acc_train: 0.6929 loss_val: 1.3715 acc_val: 0.6033 time: 0.0081s\n",
      "Epoch: 0056 loss_train: 1.2603 acc_train: 0.6214 loss_val: 1.3576 acc_val: 0.6067 time: 0.0080s\n",
      "Epoch: 0057 loss_train: 1.2333 acc_train: 0.6500 loss_val: 1.3444 acc_val: 0.6100 time: 0.0089s\n",
      "Epoch: 0058 loss_train: 1.1893 acc_train: 0.6214 loss_val: 1.3315 acc_val: 0.6167 time: 0.0083s\n",
      "Epoch: 0059 loss_train: 1.2266 acc_train: 0.6714 loss_val: 1.3191 acc_val: 0.6300 time: 0.0082s\n",
      "Epoch: 0060 loss_train: 1.1661 acc_train: 0.6714 loss_val: 1.3070 acc_val: 0.6400 time: 0.0092s\n",
      "Epoch: 0061 loss_train: 1.1778 acc_train: 0.6500 loss_val: 1.2953 acc_val: 0.6667 time: 0.0082s\n",
      "Epoch: 0062 loss_train: 1.1447 acc_train: 0.7357 loss_val: 1.2835 acc_val: 0.6833 time: 0.0087s\n",
      "Epoch: 0063 loss_train: 1.1128 acc_train: 0.6929 loss_val: 1.2719 acc_val: 0.6833 time: 0.0102s\n",
      "Epoch: 0064 loss_train: 1.0938 acc_train: 0.7071 loss_val: 1.2603 acc_val: 0.6900 time: 0.0088s\n",
      "Epoch: 0065 loss_train: 1.0846 acc_train: 0.7286 loss_val: 1.2492 acc_val: 0.7067 time: 0.0085s\n",
      "Epoch: 0066 loss_train: 1.0813 acc_train: 0.7286 loss_val: 1.2376 acc_val: 0.7067 time: 0.0089s\n",
      "Epoch: 0067 loss_train: 1.0560 acc_train: 0.7500 loss_val: 1.2251 acc_val: 0.7100 time: 0.0093s\n",
      "Epoch: 0068 loss_train: 1.0398 acc_train: 0.8000 loss_val: 1.2122 acc_val: 0.7100 time: 0.0086s\n",
      "Epoch: 0069 loss_train: 1.0486 acc_train: 0.7714 loss_val: 1.2000 acc_val: 0.7133 time: 0.0084s\n",
      "Epoch: 0070 loss_train: 1.0065 acc_train: 0.7857 loss_val: 1.1885 acc_val: 0.7200 time: 0.0084s\n",
      "Epoch: 0071 loss_train: 0.9741 acc_train: 0.7500 loss_val: 1.1774 acc_val: 0.7367 time: 0.0084s\n",
      "Epoch: 0072 loss_train: 1.0135 acc_train: 0.7857 loss_val: 1.1664 acc_val: 0.7433 time: 0.0090s\n",
      "Epoch: 0073 loss_train: 1.0162 acc_train: 0.7929 loss_val: 1.1553 acc_val: 0.7500 time: 0.0085s\n",
      "Epoch: 0074 loss_train: 0.9636 acc_train: 0.8286 loss_val: 1.1441 acc_val: 0.7533 time: 0.0081s\n",
      "Epoch: 0075 loss_train: 0.9460 acc_train: 0.7571 loss_val: 1.1332 acc_val: 0.7533 time: 0.0086s\n",
      "Epoch: 0076 loss_train: 0.9568 acc_train: 0.8000 loss_val: 1.1232 acc_val: 0.7533 time: 0.0089s\n",
      "Epoch: 0077 loss_train: 0.9186 acc_train: 0.7714 loss_val: 1.1132 acc_val: 0.7533 time: 0.0082s\n",
      "Epoch: 0078 loss_train: 0.9226 acc_train: 0.8071 loss_val: 1.1033 acc_val: 0.7600 time: 0.0086s\n",
      "Epoch: 0079 loss_train: 0.9028 acc_train: 0.8071 loss_val: 1.0940 acc_val: 0.7600 time: 0.0086s\n",
      "Epoch: 0080 loss_train: 0.9136 acc_train: 0.8000 loss_val: 1.0848 acc_val: 0.7567 time: 0.0082s\n",
      "Epoch: 0081 loss_train: 0.9408 acc_train: 0.8143 loss_val: 1.0756 acc_val: 0.7600 time: 0.0087s\n",
      "Epoch: 0082 loss_train: 0.8838 acc_train: 0.8000 loss_val: 1.0667 acc_val: 0.7700 time: 0.0090s\n",
      "Epoch: 0083 loss_train: 0.8190 acc_train: 0.8500 loss_val: 1.0584 acc_val: 0.7833 time: 0.0083s\n",
      "Epoch: 0084 loss_train: 0.8884 acc_train: 0.7786 loss_val: 1.0512 acc_val: 0.7700 time: 0.0086s\n",
      "Epoch: 0085 loss_train: 0.8399 acc_train: 0.8143 loss_val: 1.0431 acc_val: 0.7767 time: 0.0086s\n",
      "Epoch: 0086 loss_train: 0.8294 acc_train: 0.8429 loss_val: 1.0346 acc_val: 0.7800 time: 0.0097s\n",
      "Epoch: 0087 loss_train: 0.8220 acc_train: 0.8500 loss_val: 1.0261 acc_val: 0.7833 time: 0.0090s\n",
      "Epoch: 0088 loss_train: 0.8111 acc_train: 0.8571 loss_val: 1.0182 acc_val: 0.7900 time: 0.0093s\n",
      "Epoch: 0089 loss_train: 0.8510 acc_train: 0.8214 loss_val: 1.0099 acc_val: 0.7900 time: 0.0087s\n",
      "Epoch: 0090 loss_train: 0.7760 acc_train: 0.8357 loss_val: 1.0025 acc_val: 0.7900 time: 0.0081s\n",
      "Epoch: 0091 loss_train: 0.7961 acc_train: 0.8214 loss_val: 0.9959 acc_val: 0.7800 time: 0.0082s\n",
      "Epoch: 0092 loss_train: 0.7737 acc_train: 0.8429 loss_val: 0.9900 acc_val: 0.7800 time: 0.0090s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0093 loss_train: 0.7738 acc_train: 0.8500 loss_val: 0.9846 acc_val: 0.7767 time: 0.0089s\n",
      "Epoch: 0094 loss_train: 0.7993 acc_train: 0.8643 loss_val: 0.9802 acc_val: 0.7800 time: 0.0089s\n",
      "Epoch: 0095 loss_train: 0.7463 acc_train: 0.8429 loss_val: 0.9755 acc_val: 0.7767 time: 0.0090s\n",
      "Epoch: 0096 loss_train: 0.7820 acc_train: 0.8786 loss_val: 0.9700 acc_val: 0.7767 time: 0.0076s\n",
      "Epoch: 0097 loss_train: 0.7639 acc_train: 0.8214 loss_val: 0.9631 acc_val: 0.7800 time: 0.0088s\n",
      "Epoch: 0098 loss_train: 0.7594 acc_train: 0.8500 loss_val: 0.9560 acc_val: 0.7833 time: 0.0088s\n",
      "Epoch: 0099 loss_train: 0.7147 acc_train: 0.8643 loss_val: 0.9490 acc_val: 0.7867 time: 0.0086s\n",
      "Epoch: 0100 loss_train: 0.7394 acc_train: 0.8214 loss_val: 0.9435 acc_val: 0.7867 time: 0.0085s\n",
      "Epoch: 0101 loss_train: 0.7543 acc_train: 0.8714 loss_val: 0.9377 acc_val: 0.7867 time: 0.0085s\n",
      "Epoch: 0102 loss_train: 0.7172 acc_train: 0.8571 loss_val: 0.9319 acc_val: 0.7867 time: 0.0081s\n",
      "Epoch: 0103 loss_train: 0.7278 acc_train: 0.8643 loss_val: 0.9263 acc_val: 0.7833 time: 0.0087s\n",
      "Epoch: 0104 loss_train: 0.7044 acc_train: 0.8571 loss_val: 0.9209 acc_val: 0.7833 time: 0.0086s\n",
      "Epoch: 0105 loss_train: 0.6642 acc_train: 0.9143 loss_val: 0.9160 acc_val: 0.7833 time: 0.0071s\n",
      "Epoch: 0106 loss_train: 0.7460 acc_train: 0.8214 loss_val: 0.9112 acc_val: 0.7833 time: 0.0089s\n",
      "Epoch: 0107 loss_train: 0.6756 acc_train: 0.8786 loss_val: 0.9069 acc_val: 0.7833 time: 0.0086s\n",
      "Epoch: 0108 loss_train: 0.7050 acc_train: 0.9000 loss_val: 0.9030 acc_val: 0.7800 time: 0.0084s\n",
      "Epoch: 0109 loss_train: 0.7186 acc_train: 0.8500 loss_val: 0.9004 acc_val: 0.7767 time: 0.0092s\n",
      "Epoch: 0110 loss_train: 0.7085 acc_train: 0.8714 loss_val: 0.8975 acc_val: 0.7767 time: 0.0087s\n",
      "Epoch: 0111 loss_train: 0.6879 acc_train: 0.8429 loss_val: 0.8940 acc_val: 0.7767 time: 0.0081s\n",
      "Epoch: 0112 loss_train: 0.6528 acc_train: 0.8857 loss_val: 0.8905 acc_val: 0.7800 time: 0.0086s\n",
      "Epoch: 0113 loss_train: 0.6451 acc_train: 0.8714 loss_val: 0.8863 acc_val: 0.7800 time: 0.0087s\n",
      "Epoch: 0114 loss_train: 0.6277 acc_train: 0.9000 loss_val: 0.8823 acc_val: 0.7800 time: 0.0082s\n",
      "Epoch: 0115 loss_train: 0.6317 acc_train: 0.9143 loss_val: 0.8775 acc_val: 0.7800 time: 0.0088s\n",
      "Epoch: 0116 loss_train: 0.6443 acc_train: 0.8714 loss_val: 0.8738 acc_val: 0.7900 time: 0.0091s\n",
      "Epoch: 0117 loss_train: 0.6079 acc_train: 0.9000 loss_val: 0.8715 acc_val: 0.7900 time: 0.0084s\n",
      "Epoch: 0118 loss_train: 0.6205 acc_train: 0.8857 loss_val: 0.8695 acc_val: 0.7900 time: 0.0087s\n",
      "Epoch: 0119 loss_train: 0.6197 acc_train: 0.8857 loss_val: 0.8660 acc_val: 0.7933 time: 0.0087s\n",
      "Epoch: 0120 loss_train: 0.6152 acc_train: 0.8857 loss_val: 0.8622 acc_val: 0.7933 time: 0.0085s\n",
      "Epoch: 0121 loss_train: 0.6250 acc_train: 0.8714 loss_val: 0.8575 acc_val: 0.7900 time: 0.0085s\n",
      "Epoch: 0122 loss_train: 0.6125 acc_train: 0.8929 loss_val: 0.8527 acc_val: 0.7900 time: 0.0085s\n",
      "Epoch: 0123 loss_train: 0.6302 acc_train: 0.8643 loss_val: 0.8487 acc_val: 0.7933 time: 0.0083s\n",
      "Epoch: 0124 loss_train: 0.6121 acc_train: 0.8857 loss_val: 0.8464 acc_val: 0.7900 time: 0.0090s\n",
      "Epoch: 0125 loss_train: 0.6063 acc_train: 0.9071 loss_val: 0.8443 acc_val: 0.7867 time: 0.0089s\n",
      "Epoch: 0126 loss_train: 0.6006 acc_train: 0.9000 loss_val: 0.8407 acc_val: 0.7867 time: 0.0081s\n",
      "Epoch: 0127 loss_train: 0.5829 acc_train: 0.9143 loss_val: 0.8360 acc_val: 0.7933 time: 0.0088s\n",
      "Epoch: 0128 loss_train: 0.6303 acc_train: 0.8786 loss_val: 0.8321 acc_val: 0.7900 time: 0.0087s\n",
      "Epoch: 0129 loss_train: 0.6014 acc_train: 0.8857 loss_val: 0.8280 acc_val: 0.7933 time: 0.0082s\n",
      "Epoch: 0130 loss_train: 0.6030 acc_train: 0.8857 loss_val: 0.8232 acc_val: 0.7967 time: 0.0090s\n",
      "Epoch: 0131 loss_train: 0.5690 acc_train: 0.8714 loss_val: 0.8187 acc_val: 0.8033 time: 0.0086s\n",
      "Epoch: 0132 loss_train: 0.5324 acc_train: 0.9143 loss_val: 0.8139 acc_val: 0.8000 time: 0.0081s\n",
      "Epoch: 0133 loss_train: 0.5241 acc_train: 0.9214 loss_val: 0.8101 acc_val: 0.8033 time: 0.0085s\n",
      "Epoch: 0134 loss_train: 0.5799 acc_train: 0.9000 loss_val: 0.8076 acc_val: 0.8033 time: 0.0087s\n",
      "Epoch: 0135 loss_train: 0.5743 acc_train: 0.9000 loss_val: 0.8066 acc_val: 0.8033 time: 0.0082s\n",
      "Epoch: 0136 loss_train: 0.5543 acc_train: 0.9071 loss_val: 0.8051 acc_val: 0.8033 time: 0.0088s\n",
      "Epoch: 0137 loss_train: 0.5372 acc_train: 0.9071 loss_val: 0.8045 acc_val: 0.8067 time: 0.0091s\n",
      "Epoch: 0138 loss_train: 0.5797 acc_train: 0.8786 loss_val: 0.8018 acc_val: 0.8033 time: 0.0081s\n",
      "Epoch: 0139 loss_train: 0.5689 acc_train: 0.8643 loss_val: 0.7980 acc_val: 0.8033 time: 0.0091s\n",
      "Epoch: 0140 loss_train: 0.5428 acc_train: 0.8857 loss_val: 0.7939 acc_val: 0.8067 time: 0.0090s\n",
      "Epoch: 0141 loss_train: 0.5695 acc_train: 0.8857 loss_val: 0.7897 acc_val: 0.8067 time: 0.0088s\n",
      "Epoch: 0142 loss_train: 0.5144 acc_train: 0.9286 loss_val: 0.7872 acc_val: 0.8067 time: 0.0087s\n",
      "Epoch: 0143 loss_train: 0.5451 acc_train: 0.9214 loss_val: 0.7856 acc_val: 0.8033 time: 0.0087s\n",
      "Epoch: 0144 loss_train: 0.5403 acc_train: 0.8714 loss_val: 0.7846 acc_val: 0.8033 time: 0.0082s\n",
      "Epoch: 0145 loss_train: 0.5122 acc_train: 0.9357 loss_val: 0.7829 acc_val: 0.8033 time: 0.0089s\n",
      "Epoch: 0146 loss_train: 0.5150 acc_train: 0.9071 loss_val: 0.7812 acc_val: 0.8067 time: 0.0085s\n",
      "Epoch: 0147 loss_train: 0.5135 acc_train: 0.9071 loss_val: 0.7787 acc_val: 0.8033 time: 0.0084s\n",
      "Epoch: 0148 loss_train: 0.5090 acc_train: 0.9286 loss_val: 0.7760 acc_val: 0.8033 time: 0.0087s\n",
      "Epoch: 0149 loss_train: 0.5020 acc_train: 0.9214 loss_val: 0.7725 acc_val: 0.8000 time: 0.0092s\n",
      "Epoch: 0150 loss_train: 0.4947 acc_train: 0.9000 loss_val: 0.7703 acc_val: 0.7967 time: 0.0083s\n",
      "Epoch: 0151 loss_train: 0.5090 acc_train: 0.8929 loss_val: 0.7685 acc_val: 0.8000 time: 0.0086s\n",
      "Epoch: 0152 loss_train: 0.5077 acc_train: 0.9143 loss_val: 0.7670 acc_val: 0.8000 time: 0.0085s\n",
      "Epoch: 0153 loss_train: 0.4860 acc_train: 0.9071 loss_val: 0.7640 acc_val: 0.8000 time: 0.0084s\n",
      "Epoch: 0154 loss_train: 0.4833 acc_train: 0.9500 loss_val: 0.7603 acc_val: 0.8000 time: 0.0087s\n",
      "Epoch: 0155 loss_train: 0.5123 acc_train: 0.9429 loss_val: 0.7562 acc_val: 0.8000 time: 0.0088s\n",
      "Epoch: 0156 loss_train: 0.5035 acc_train: 0.8929 loss_val: 0.7532 acc_val: 0.8033 time: 0.0086s\n",
      "Epoch: 0157 loss_train: 0.4937 acc_train: 0.9000 loss_val: 0.7520 acc_val: 0.8033 time: 0.0085s\n",
      "Epoch: 0158 loss_train: 0.4774 acc_train: 0.9143 loss_val: 0.7533 acc_val: 0.8000 time: 0.0085s\n",
      "Epoch: 0159 loss_train: 0.4679 acc_train: 0.9357 loss_val: 0.7531 acc_val: 0.8000 time: 0.0081s\n",
      "Epoch: 0160 loss_train: 0.4599 acc_train: 0.9357 loss_val: 0.7518 acc_val: 0.8000 time: 0.0087s\n",
      "Epoch: 0161 loss_train: 0.4486 acc_train: 0.9286 loss_val: 0.7516 acc_val: 0.7967 time: 0.0086s\n",
      "Epoch: 0162 loss_train: 0.4532 acc_train: 0.9429 loss_val: 0.7507 acc_val: 0.7967 time: 0.0083s\n",
      "Epoch: 0163 loss_train: 0.4539 acc_train: 0.9143 loss_val: 0.7489 acc_val: 0.7933 time: 0.0088s\n",
      "Epoch: 0164 loss_train: 0.4667 acc_train: 0.9214 loss_val: 0.7476 acc_val: 0.7900 time: 0.0086s\n",
      "Epoch: 0165 loss_train: 0.4854 acc_train: 0.9000 loss_val: 0.7444 acc_val: 0.7933 time: 0.0084s\n",
      "Epoch: 0166 loss_train: 0.4681 acc_train: 0.9500 loss_val: 0.7425 acc_val: 0.8000 time: 0.0091s\n",
      "Epoch: 0167 loss_train: 0.4410 acc_train: 0.9214 loss_val: 0.7404 acc_val: 0.8067 time: 0.0089s\n",
      "Epoch: 0168 loss_train: 0.4699 acc_train: 0.9429 loss_val: 0.7384 acc_val: 0.8067 time: 0.0081s\n",
      "Epoch: 0169 loss_train: 0.4355 acc_train: 0.9357 loss_val: 0.7358 acc_val: 0.8067 time: 0.0084s\n",
      "Epoch: 0170 loss_train: 0.4824 acc_train: 0.9143 loss_val: 0.7336 acc_val: 0.8067 time: 0.0088s\n",
      "Epoch: 0171 loss_train: 0.4867 acc_train: 0.9357 loss_val: 0.7317 acc_val: 0.8100 time: 0.0086s\n",
      "Epoch: 0172 loss_train: 0.4728 acc_train: 0.9000 loss_val: 0.7307 acc_val: 0.8100 time: 0.0086s\n",
      "Epoch: 0173 loss_train: 0.4491 acc_train: 0.8929 loss_val: 0.7307 acc_val: 0.8067 time: 0.0085s\n",
      "Epoch: 0174 loss_train: 0.4551 acc_train: 0.9000 loss_val: 0.7309 acc_val: 0.8067 time: 0.0084s\n",
      "Epoch: 0175 loss_train: 0.4326 acc_train: 0.9357 loss_val: 0.7314 acc_val: 0.8067 time: 0.0088s\n",
      "Epoch: 0176 loss_train: 0.4283 acc_train: 0.9071 loss_val: 0.7325 acc_val: 0.8033 time: 0.0087s\n",
      "Epoch: 0177 loss_train: 0.4449 acc_train: 0.9357 loss_val: 0.7312 acc_val: 0.8033 time: 0.0086s\n",
      "Epoch: 0178 loss_train: 0.4413 acc_train: 0.9429 loss_val: 0.7286 acc_val: 0.8033 time: 0.0085s\n",
      "Epoch: 0179 loss_train: 0.4454 acc_train: 0.9357 loss_val: 0.7256 acc_val: 0.8000 time: 0.0085s\n",
      "Epoch: 0180 loss_train: 0.3917 acc_train: 0.9357 loss_val: 0.7224 acc_val: 0.8000 time: 0.0082s\n",
      "Epoch: 0181 loss_train: 0.4118 acc_train: 0.9429 loss_val: 0.7210 acc_val: 0.8067 time: 0.0089s\n",
      "Epoch: 0182 loss_train: 0.4371 acc_train: 0.9071 loss_val: 0.7197 acc_val: 0.8067 time: 0.0088s\n",
      "Epoch: 0183 loss_train: 0.3908 acc_train: 0.9571 loss_val: 0.7187 acc_val: 0.8100 time: 0.0082s\n",
      "Epoch: 0184 loss_train: 0.4156 acc_train: 0.9429 loss_val: 0.7186 acc_val: 0.8067 time: 0.0088s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0185 loss_train: 0.4353 acc_train: 0.9286 loss_val: 0.7202 acc_val: 0.8067 time: 0.0091s\n",
      "Epoch: 0186 loss_train: 0.4033 acc_train: 0.9429 loss_val: 0.7229 acc_val: 0.8067 time: 0.0085s\n",
      "Epoch: 0187 loss_train: 0.3994 acc_train: 0.9214 loss_val: 0.7252 acc_val: 0.8067 time: 0.0089s\n",
      "Epoch: 0188 loss_train: 0.4137 acc_train: 0.9429 loss_val: 0.7235 acc_val: 0.8067 time: 0.0087s\n",
      "Epoch: 0189 loss_train: 0.4144 acc_train: 0.9214 loss_val: 0.7187 acc_val: 0.8067 time: 0.0086s\n",
      "Epoch: 0190 loss_train: 0.3750 acc_train: 0.9357 loss_val: 0.7133 acc_val: 0.8067 time: 0.0082s\n",
      "Epoch: 0191 loss_train: 0.3957 acc_train: 0.9357 loss_val: 0.7098 acc_val: 0.8067 time: 0.0090s\n",
      "Epoch: 0192 loss_train: 0.4196 acc_train: 0.9429 loss_val: 0.7082 acc_val: 0.8067 time: 0.0088s\n",
      "Epoch: 0193 loss_train: 0.4143 acc_train: 0.9286 loss_val: 0.7072 acc_val: 0.8067 time: 0.0084s\n",
      "Epoch: 0194 loss_train: 0.4256 acc_train: 0.9214 loss_val: 0.7062 acc_val: 0.8067 time: 0.0090s\n",
      "Epoch: 0195 loss_train: 0.3926 acc_train: 0.9286 loss_val: 0.7070 acc_val: 0.8067 time: 0.0087s\n",
      "Epoch: 0196 loss_train: 0.4034 acc_train: 0.9286 loss_val: 0.7096 acc_val: 0.8067 time: 0.0082s\n",
      "Epoch: 0197 loss_train: 0.3873 acc_train: 0.9429 loss_val: 0.7122 acc_val: 0.8067 time: 0.0089s\n",
      "Epoch: 0198 loss_train: 0.4118 acc_train: 0.9714 loss_val: 0.7136 acc_val: 0.8067 time: 0.0085s\n",
      "Epoch: 0199 loss_train: 0.4182 acc_train: 0.9571 loss_val: 0.7117 acc_val: 0.8067 time: 0.0082s\n",
      "Epoch: 0200 loss_train: 0.4006 acc_train: 0.9500 loss_val: 0.7091 acc_val: 0.8067 time: 0.0087s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 2.0712s\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "t_total = time.time()\n",
    "for epoch in range(epochs):\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: loss= 0.7418 accuracy= 0.8200\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
